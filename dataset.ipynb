{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install pandas matplotlib tensorflow matplotlib huggingface datasets dlib scikit-learn albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset ,load_from_disk\n",
    "import dlib\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import albumentations as alb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlib_detector(img):\n",
    "    dlib_face_detector = dlib.get_frontal_face_detector()\n",
    "    dlib_face_locations = dlib_face_detector(img)\n",
    "    return dlib_face_locations  # xmin,ymin,xmax,ymax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resize all images to same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target size for all images\n",
    "TARGET_SIZE = (640, 480)  # (width, height)\n",
    "\n",
    "input_dir = './Dataset_raw/'\n",
    "output_dir = './Dataset_resized/'\n",
    "\n",
    "# Ensure output directories exist\n",
    "for class_name in os.listdir(input_dir):\n",
    "    class_path = os.path.join(output_dir, class_name)\n",
    "    os.makedirs(class_path, exist_ok=True)\n",
    "\n",
    "for class_name in os.listdir(input_dir):\n",
    "    class_path = os.path.join(input_dir, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "    \n",
    "    for image_name in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "        \n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Error reading {image_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Get the original dimensions\n",
    "            h, w = image.shape[:2]\n",
    "            \n",
    "            # Calculate the aspect ratio and resize accordingly\n",
    "            scale = min(TARGET_SIZE[1] / h, TARGET_SIZE[0] / w)\n",
    "            new_w = int(w * scale)\n",
    "            new_h = int(h * scale)\n",
    "            \n",
    "            resized_image = cv2.resize(image, (new_w, new_h))  # Resize while keeping aspect ratio\n",
    "            \n",
    "            # Create a black canvas of the target size\n",
    "            canvas = cv2.copyMakeBorder(\n",
    "                resized_image, \n",
    "                (TARGET_SIZE[1] - new_h) // 2,  # Top padding\n",
    "                (TARGET_SIZE[1] - new_h + 1) // 2,  # Bottom padding\n",
    "                (TARGET_SIZE[0] - new_w) // 2,  # Left padding\n",
    "                (TARGET_SIZE[0] - new_w + 1) // 2,  # Right padding\n",
    "                cv2.BORDER_CONSTANT,\n",
    "                value=(0, 0, 0)  # Black padding\n",
    "            )\n",
    "\n",
    "            save_path = os.path.join(output_dir, class_name, image_name)\n",
    "            cv2.imwrite(save_path, canvas)  # Save the padded image\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "print(\"Image resizing with aspect ratio preservation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data to train test val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "input_dir = './Dataset_resized/'\n",
    "output_dir = './Dataset_split/'\n",
    "\n",
    "# Define split ratios (can be changed as needed)\n",
    "train_split = 0.65\n",
    "val_split = 0.25\n",
    "test_split = 0.1\n",
    "\n",
    "# Ensure the output directories exist\n",
    "splits = ['train', 'val', 'test']\n",
    "for split in splits:\n",
    "    for class_name in os.listdir(input_dir):\n",
    "        split_dir = os.path.join(output_dir, split, class_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "# Process each folder (person1, person2, etc.)\n",
    "for class_name in os.listdir(input_dir):\n",
    "    class_path = os.path.join(input_dir, class_name)\n",
    "    \n",
    "    if not os.path.isdir(class_path):\n",
    "        continue  # Skip files, only process directories (like person1, person2)\n",
    "    \n",
    "    # Get all image files in the current class folder\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "    random.shuffle(images)  # Shuffle to ensure randomness\n",
    "\n",
    "    # Split images into train, val, and test\n",
    "    total_images = len(images)\n",
    "    train_count = int(total_images * train_split)\n",
    "    val_count = int(total_images * val_split)\n",
    "    \n",
    "    train_images = images[:train_count]\n",
    "    val_images = images[train_count:train_count + val_count]\n",
    "    test_images = images[train_count + val_count:]\n",
    "\n",
    "    # Copy files to train, val, and test directories\n",
    "    for image_set, split in zip([train_images, val_images, test_images], splits):\n",
    "        for image_name in image_set:\n",
    "            src_path = os.path.join(class_path, image_name)\n",
    "            dst_path = os.path.join(output_dir, split, class_name, image_name)\n",
    "            \n",
    "            try:\n",
    "                shutil.copy(src_path, dst_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {src_path} to {dst_path}: {e}\")\n",
    "\n",
    "print(\"Dataset successfully organized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## augment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Intern\\Gemicats\\Deep-Face-detection-tf-data_preparation\\venv\\lib\\site-packages\\albumentations\\core\\bbox_utils.py:478: RuntimeWarning: invalid value encountered in divide\n",
      "  & (clipped_box_areas / denormalized_box_areas >= min_visibility - epsilon)\n"
     ]
    }
   ],
   "source": [
    "# Define the augmentor\n",
    "augmentor = alb.Compose([\n",
    "    alb.RandomCrop(width=450, height=450), \n",
    "    alb.HorizontalFlip(p=0.5), \n",
    "    alb.RandomBrightnessContrast(p=0.2),\n",
    "    alb.RandomGamma(p=0.2), \n",
    "    alb.RGBShift(p=0.2), \n",
    "    alb.VerticalFlip(p=0.5)\n",
    "], bbox_params=alb.BboxParams(format='albumentations', label_fields=['class_labels']))\n",
    "\n",
    "def create_class_dict(input_dir='Dataset_split'):\n",
    "    person_names = set()  \n",
    "    for partition in ['train', 'test', 'val']:\n",
    "        partition_path = os.path.join(input_dir, partition)\n",
    "        for person_folder in os.listdir(partition_path):\n",
    "            person_path = os.path.join(partition_path, person_folder)\n",
    "            if os.path.isdir(person_path):\n",
    "                person_names.add(person_folder)\n",
    "    person_names = sorted(person_names)\n",
    "    class_dict = {person_name: idx for idx, person_name in enumerate(person_names)}\n",
    "    with open('class_dict.json', 'w') as f:\n",
    "        json.dump(class_dict, f, indent=4)\n",
    "    return class_dict\n",
    "\n",
    "def create_aug_data(input_dir='Dataset_split', output_dir='aug_data'):\n",
    "    class_dict = create_class_dict(input_dir)\n",
    "\n",
    "    for partition in ['train', 'test', 'val']:\n",
    "        image_dir = os.path.join(output_dir, partition, 'images')\n",
    "        label_dir = os.path.join(output_dir, partition, 'labels')\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "    \n",
    "    for partition in ['train', 'test', 'val']:\n",
    "        partition_path = os.path.join(input_dir, partition)\n",
    "        \n",
    "        for person_folder in os.listdir(partition_path):\n",
    "            person_path = os.path.join(partition_path, person_folder)\n",
    "            \n",
    "            if not os.path.isdir(person_path):\n",
    "                continue\n",
    "            \n",
    "            class_label = class_dict[person_folder]\n",
    "            \n",
    "            for image_name in os.listdir(person_path):\n",
    "                image_path = os.path.join(person_path, image_name)\n",
    "                \n",
    "                try:\n",
    "                    img = cv2.imread(image_path)  \n",
    "                    if img is None:\n",
    "                        print(f\"Error reading image: {image_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    height, width = img.shape[:2]\n",
    "                    image_numpy = np.array(img)\n",
    "                    # Assuming dlib_detector returns a dlib.rectangle\n",
    "                    dlib_rect = dlib_detector(image_numpy)\n",
    "                    # Extract the coordinates from the dlib rectangle\n",
    "                    x_min = dlib_rect[0].left()\n",
    "                    y_min = dlib_rect[0].top()\n",
    "                    x_max = dlib_rect[0].right()\n",
    "                    y_max = dlib_rect[0].bottom()\n",
    "\n",
    "                    # Normalize the coordinates relative to the image size\n",
    "                    coords = [\n",
    "                        x_min / width,  \n",
    "                        y_min / height, \n",
    "                        x_max / width,  \n",
    "                        y_max / height  \n",
    "                    ]\n",
    "                                        \n",
    "                    for x in range(60):  # Generate 60 augmented versions for each image\n",
    "                        augmented = augmentor(\n",
    "                            image=img, \n",
    "                            bboxes=[coords], \n",
    "                            class_labels=[class_label]\n",
    "                        )\n",
    "                        \n",
    "                        aug_image_name = f'{os.path.splitext(image_name)[0]}.{x}.jpg'\n",
    "                        aug_image_path = os.path.join(output_dir, partition, 'images', aug_image_name)\n",
    "                        cv2.imwrite(aug_image_path, augmented['image'])\n",
    "                        \n",
    "                        if len(augmented['bboxes']) > 0:\n",
    "                            updated_bbox = augmented['bboxes'][0]\n",
    "                        else:\n",
    "                            updated_bbox = [0, 0, 0, 0]  # If no bbox, set it to zero\n",
    "                        \n",
    "                        annotation = {\n",
    "                            \"image\": aug_image_name,\n",
    "                            \"bbox\": updated_bbox,\n",
    "                            \"class\": class_label  # Class label for this person\n",
    "                        }\n",
    "                        \n",
    "                        aug_json_name = f'{os.path.splitext(image_name)[0]}.{x}.json'\n",
    "                        aug_json_path = os.path.join(output_dir, partition, 'labels', aug_json_name)\n",
    "                        with open(aug_json_path, 'w') as json_file:\n",
    "                            json.dump(annotation, json_file)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "create_aug_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
